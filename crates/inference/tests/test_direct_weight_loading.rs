//! Direct test of TinyLlama weight loading

use std::path::PathBuf;

#[cfg(feature = "burn-cpu")]
#[tokio::test] 
async fn test_direct_weight_loading() -> Result<(), Box<dyn std::error::Error>> {
    println!("ğŸ”§ Testing direct TinyLlama weight loading");
    
    let model_path = PathBuf::from("../../models/tinyllama-1.1b");
    println!("ğŸ“ Model path: {:?}", model_path);
    
    // Check if files exist
    let weights_file = model_path.join("model.safetensors");
    let tokenizer_file = model_path.join("tokenizer.json");
    
    println!("ğŸ“„ Weights file exists: {}", weights_file.exists());
    println!("ğŸ“„ Tokenizer file exists: {}", tokenizer_file.exists());
    
    if !weights_file.exists() || !tokenizer_file.exists() {
        println!("âš ï¸  Required files missing, skipping weight loading test");
        return Ok(());
    }
    
    // Create device
    let device = burn::backend::ndarray::NdArrayDevice::default();
    println!("ğŸ–¥ï¸ Device created: {:?}", device);
    
    // Try to load weights directly
    println!("ğŸ”„ Attempting to load weights...");
    
    match inferno_inference::models::load_llama_weights(&model_path, &device) {
        Ok(model) => {
            println!("âœ… Successfully loaded TinyLlama model!");
            println!("ğŸ“Š Model loaded - this indicates burn-llama is working!");
            
            // Basic validation
            println!("ğŸ¯ Model loaded successfully with burn-llama integration");
        }
        Err(e) => {
            println!("âŒ Failed to load model: {}", e);
            println!("This shows what specific error we're encountering");
        }
    }
    
    println!("âœ… Direct weight loading test completed");
    Ok(())
}