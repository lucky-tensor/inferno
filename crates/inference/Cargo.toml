[package]
name = "inferno-inference"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
description = "Inference engine implementation for Inferno with Burn ML framework support"

[[example]]
name = "simple_inference"
path = "examples/simple_inference.rs"

[[example]]
name = "concurrent_inference"
path = "examples/concurrent_inference.rs"

[[example]]
name = "inspect_safetensors"
path = "examples/inspect_safetensors.rs"


[dependencies]
# Workspace dependencies
tokio = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
tracing = { workspace = true }
thiserror = { workspace = true }
async-trait = { workspace = true }
chrono = { workspace = true }

# Local dependencies
inferno-shared = { workspace = true }

# Configuration and environment
config = { workspace = true }
toml = { workspace = true }
num_cpus = { workspace = true }

# CLI dependencies (for examples)
clap = { version = "4.0", features = ["derive"], optional = true }
tracing-subscriber = { version = "0.3", optional = true }


# JSON and validation
validator = { workspace = true }

# Process execution is handled by std::process (no additional crate needed)

# Burn ML Framework for real model inference
burn = { workspace = true, optional = true }
burn-import = { workspace = true, optional = true }
llama-burn = { workspace = true, optional = true }

# Candle ML Framework for optimized inference (with CUDA 12.9 support)
candle-core = { version = "0.9.1", optional = true }
candle-nn = { version = "0.9.1", optional = true }
candle-transformers = { version = "0.9.1", optional = true }

# Shared dependencies for both frameworks
tokenizers = { workspace = true, optional = true }
hf-hub = { workspace = true, optional = true }
safetensors = { workspace = true, optional = true }
bytemuck = { version = "1.0", optional = true }
half = { version = "2.3", optional = true }


[dev-dependencies]
tokio-test = { workspace = true }
criterion = { workspace = true }
proptest = { workspace = true }
serial_test = { workspace = true }
tempfile = { workspace = true }
rstest = { workspace = true }

[features]
default = ["candle-cuda", "pretrained"]
pretrained = []
examples = ["dep:clap", "dep:tracing-subscriber"]

# Burn ML Framework features
burn-cpu = [
    "burn/ndarray",
    "burn/autodiff",
    "dep:tokenizers",
    "hf-hub/tokio",
    "dep:safetensors",
    "dep:burn-import",
    "dep:llama-burn"
]  # Real deterministic Llama model inference with SafeTensors support (default)

# Candle ML Framework features
candle-cpu = [
    "dep:candle-core",
    "dep:candle-nn",
    "dep:candle-transformers",
    "dep:tokenizers",
    "hf-hub/tokio",
    "dep:safetensors",
    "dep:bytemuck",
    "dep:half"
]  # HuggingFace-compatible inference with Candle framework

candle-cuda = [
    "dep:candle-core",
    "dep:candle-nn",
    "dep:candle-transformers",
    "dep:tokenizers",
    "hf-hub/tokio",
    "dep:safetensors",
    "dep:bytemuck",
    "dep:half",
    "candle-core/cuda",
    "candle-nn/cuda",
    "candle-transformers/cuda"
]  # CUDA-accelerated inference with optimized kernels

candle-metal = [
    "dep:candle-core",
    "dep:candle-nn",
    "dep:candle-transformers",
    "dep:tokenizers",
    "hf-hub/tokio",
    "dep:safetensors",
    "dep:bytemuck",
    "dep:half",
    "candle-core/metal",
    "candle-nn/metal",
    "candle-transformers/metal"
]  # Metal-accelerated inference for Apple Silicon

[[bench]]
name = "inference_benchmark"
harness = false
