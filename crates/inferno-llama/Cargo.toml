[package]
name = "inferno-llama"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
repository.workspace = true
description = "Native BF16/F16 Llama implementation for high-performance distributed systems"

[dependencies]
# Core candle dependencies for tensor operations
candle-core = { version = "0.9.1" }
candle-nn = { version = "0.9.1" }

# Workspace dependencies
thiserror = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }

# Math and precision support
half = { workspace = true }
bytemuck = { version = "1.0" }
regex = { version = "1.0" }

# For configuration loading
hf-hub = { workspace = true }
safetensors = { workspace = true }

# For tokenization support
tokenizers = { workspace = true }
tracing = { workspace = true }
tokio = { workspace = true }

[dev-dependencies]
criterion = { workspace = true }
proptest = { workspace = true }
tokio-test = { workspace = true }
tokio = { workspace = true }
rstest = { workspace = true }
tempfile = { workspace = true }

[features]
default = ["cuda"]
cuda = ["candle-core/cuda", "candle-nn/cuda"]
metal = ["candle-core/metal", "candle-nn/metal"]

[package.metadata.cargo-machete]
ignored = ["bytemuck", "candle-nn", "half", "hf-hub", "safetensors", "regex"]

