//! # Simple Model Loader
//!
//! A simplified implementation of model loading focused on getting real model
//! weight loading working with minimal complexity. This focuses on the core
//! functionality needed for the end-to-end test.

use candle_core::{DType, Device, Tensor};
use candle_nn::VarMap;
use serde_json::Value;
use std::collections::HashMap;
use std::fs;
use std::path::Path;

use crate::config::LlamaConfig;
use crate::error::{LlamaError, Result};
use crate::model::InfernoLlama;

impl InfernoLlama {
    /// Loads a pre-trained Llama model from the specified path using a simple approach.
    ///
    /// NOTE: This method has been replaced by the real load_from_path implementation
    /// in model.rs that includes weight analysis and dtype preservation.
    /// This method is kept for reference but renamed to avoid conflicts.
    pub fn load_from_path_simple<P: AsRef<Path>>(model_path: P) -> Result<Self> {
        let model_path = model_path.as_ref();

        // Load configuration
        let config = Self::load_config_simple(model_path)?;

        // For now, create an empty model to test the basic structure
        // We'll implement weight loading in the next step
        let device = Device::Cpu;
        let dtype = DType::BF16;

        // Create an empty VarMap for initial implementation
        let var_map = candle_nn::VarMap::new();
        let vb = candle_nn::VarBuilder::from_varmap(&var_map, dtype, &device);

        // Create model structure
        let model = Self::new(&config, vb)?;

        Ok(model)
    }

    /// Loads a model with actual weights from SafeTensors files.
    ///
    /// This method loads the real model weights instead of creating empty parameters.
    /// For now, this is a placeholder that creates an empty model for structural testing.
    /// The actual weight loading will be implemented in the next iteration.
    pub fn load_from_path_with_weights<P: AsRef<Path>>(model_path: P) -> Result<Self> {
        let model_path = model_path.as_ref();

        // Load configuration
        let config = Self::load_config_simple(model_path)?;

        // For now, just create an empty model to validate the structure
        // This will be enhanced in future iterations to actually load weights
        let device = Device::Cpu;
        let dtype = DType::BF16;

        // Create an empty VarMap
        let var_map = VarMap::new();
        let vb = candle_nn::VarBuilder::from_varmap(&var_map, dtype, &device);

        // Create model structure (weights will be randomly initialized)
        let model = Self::new(&config, vb)?;

        println!("Model created successfully with BF16 precision. Weight loading from SafeTensors is validated in other tests.");

        Ok(model)
    }

    /// Loads weight tensors from SafeTensors files in the model directory.
    ///
    /// This function discovers all .safetensors files, loads the weights,
    /// and maps them from Hugging Face naming to InfernoLlama naming.
    pub fn load_weights_from_safetensors<P: AsRef<Path>>(
        model_path: P,
    ) -> Result<HashMap<String, Tensor>> {
        let model_path = model_path.as_ref();

        // Load weight mapping from index file or single file
        let weight_mapping = Self::get_weight_mapping_flexible(model_path)?;

        let mut all_tensors = HashMap::new();
        let device = Device::Cpu;

        // Group weights by file
        let mut files_to_weights: HashMap<String, Vec<String>> = HashMap::new();
        for (hf_name, filename) in &weight_mapping {
            files_to_weights
                .entry(filename.clone())
                .or_default()
                .push(hf_name.clone());
        }

        // Load each SafeTensors file
        for (filename, weight_names) in files_to_weights {
            let file_path = model_path.join(&filename);

            if !file_path.exists() {
                return Err(LlamaError::io_error(
                    format!("SafeTensors file not found: {:?}", file_path),
                    "load_safetensors",
                ));
            }

            // Load tensors from this file
            let file_tensors = Self::load_tensors_from_file(&file_path, &weight_names, &device)?;

            // Use HuggingFace weight names directly (no mapping needed)
            for (hf_name, tensor) in file_tensors {
                all_tensors.insert(hf_name, tensor);
            }
        }

        Ok(all_tensors)
    }

    /// Gets the weight mapping from the model's index file.
    ///
    /// This parses model.safetensors.index.json to understand which weights
    /// are stored in which SafeTensors files.
    pub fn get_weight_mapping<P: AsRef<Path>>(model_path: P) -> Result<HashMap<String, String>> {
        let model_path = model_path.as_ref();
        let index_path = model_path.join("model.safetensors.index.json");

        let index_content = fs::read_to_string(&index_path).map_err(|e| {
            LlamaError::io_error(
                format!("Failed to read weight index from {:?}: {}", index_path, e),
                "load_weight_mapping",
            )
        })?;

        let index_value: Value = serde_json::from_str(&index_content).map_err(|e| {
            LlamaError::config_error(
                "model.safetensors.index.json",
                format!("Failed to parse weight index: {}", e),
            )
        })?;

        let weight_map = index_value
            .get("weight_map")
            .ok_or_else(|| {
                LlamaError::config_error(
                    "weight_map",
                    "Missing weight_map field in index file".to_string(),
                )
            })?
            .as_object()
            .ok_or_else(|| {
                LlamaError::config_error("weight_map", "weight_map must be an object".to_string())
            })?;

        let mut result = HashMap::new();
        for (weight_name, file_name) in weight_map {
            let file_name_str = file_name.as_str().ok_or_else(|| {
                LlamaError::config_error(
                    "weight_map",
                    format!("File name must be string for weight: {}", weight_name),
                )
            })?;
            result.insert(weight_name.clone(), file_name_str.to_string());
        }

        Ok(result)
    }

    /// Gets weight mapping, handling both sharded and single-file models.
    ///
    /// This method first tries to load from model.safetensors.index.json (for sharded models),
    /// and if that fails, falls back to reading directly from model.safetensors (for single-file models).
    pub fn get_weight_mapping_flexible<P: AsRef<Path>>(
        model_path: P,
    ) -> Result<HashMap<String, String>> {
        let model_path = model_path.as_ref();

        // First try to get sharded mapping
        match Self::get_weight_mapping(model_path) {
            Ok(mapping) => Ok(mapping),
            Err(_) => {
                // If no index file, check for single model.safetensors file
                let single_file = model_path.join("model.safetensors");
                if single_file.exists() {
                    // Read the single safetensors file to get all weight names
                    let buffer = fs::read(&single_file).map_err(|e| {
                        LlamaError::io_error(
                            format!(
                                "Failed to read single SafeTensors file {:?}: {}",
                                single_file, e
                            ),
                            "load_single_safetensors",
                        )
                    })?;

                    let safetensors =
                        safetensors::SafeTensors::deserialize(&buffer).map_err(|e| {
                            LlamaError::config_error(
                                "safetensors_format",
                                format!("Failed to parse SafeTensors file: {}", e),
                            )
                        })?;

                    let mut mapping = HashMap::new();
                    for tensor_name in safetensors.names() {
                        mapping.insert(tensor_name.to_string(), "model.safetensors".to_string());
                    }
                    Ok(mapping)
                } else {
                    Err(LlamaError::io_error(
                        "No model.safetensors.index.json or model.safetensors file found"
                            .to_string(),
                        "find_safetensors_files",
                    ))
                }
            }
        }
    }

    /// Loads specific tensors from a SafeTensors file.
    fn load_tensors_from_file(
        file_path: &Path,
        weight_names: &[String],
        device: &Device,
    ) -> Result<HashMap<String, Tensor>> {
        use safetensors::SafeTensors;

        let buffer = fs::read(file_path).map_err(|e| {
            LlamaError::io_error(
                format!("Failed to read SafeTensors file {:?}: {}", file_path, e),
                "load_safetensors",
            )
        })?;

        let safetensors = SafeTensors::deserialize(&buffer).map_err(|e| {
            LlamaError::tensor_error(
                format!("Failed to deserialize SafeTensors: {}", e),
                "load_safetensors",
            )
        })?;

        let mut tensors = HashMap::new();

        for weight_name in weight_names {
            let tensor_view = safetensors.tensor(weight_name).map_err(|e| {
                LlamaError::tensor_error(
                    format!(
                        "Weight {} not found in SafeTensors file: {}",
                        weight_name, e
                    ),
                    "load_safetensors",
                )
            })?;

            let tensor = Self::safetensors_view_to_tensor(tensor_view, device)?;
            tensors.insert(weight_name.clone(), tensor);
        }

        Ok(tensors)
    }

    /// Converts a SafeTensors tensor view to a Candle tensor.
    fn safetensors_view_to_tensor(
        tensor_view: safetensors::tensor::TensorView<'_>,
        device: &Device,
    ) -> Result<Tensor> {
        use safetensors::Dtype as STDtype;

        // Map SafeTensors dtype to Candle DType
        let dtype = match tensor_view.dtype() {
            STDtype::F32 => DType::F32,
            STDtype::F16 => DType::F16,
            STDtype::BF16 => DType::BF16,
            STDtype::U8 => DType::U8,
            STDtype::U32 => DType::U32,
            STDtype::I64 => DType::I64,
            _ => {
                return Err(LlamaError::tensor_error(
                    format!("Unsupported SafeTensors dtype: {:?}", tensor_view.dtype()),
                    "dtype_conversion",
                ));
            }
        };

        // Get tensor data and shape
        let data = tensor_view.data();
        let shape: Vec<usize> = tensor_view.shape().to_vec();

        // Create tensor from raw data
        let tensor = Tensor::from_raw_buffer(data, dtype, &shape, device).map_err(|e| {
            LlamaError::tensor_error(
                format!("Failed to create tensor from SafeTensors data: {}", e),
                "tensor_creation",
            )
        })?;

        Ok(tensor)
    }

    /// Simple method to generate text (placeholder for now)
    pub fn generate(&self, prompt: &str, max_tokens: usize) -> Result<String> {
        // This is a placeholder that will be implemented once we have tokenization
        // For now, just return the prompt to verify the interface works
        Ok(format!("{} [generated {} tokens]", prompt, max_tokens))
    }

    /// Load configuration from config.json with simplified parsing
    pub fn load_config_simple(model_path: &Path) -> Result<LlamaConfig> {
        let config_path = model_path.join("config.json");
        let config_content = fs::read_to_string(&config_path).map_err(|e| {
            LlamaError::io_error(
                format!("Failed to read config.json from {:?}: {}", config_path, e),
                "load_config",
            )
        })?;

        let config_value: Value = serde_json::from_str(&config_content).map_err(|e| {
            LlamaError::config_error("config.json", format!("Failed to parse config.json: {}", e))
        })?;

        Self::parse_config_from_json(config_value)
    }

    /// Parse LlamaConfig from JSON configuration
    pub fn parse_config_from_json(config: Value) -> Result<LlamaConfig> {
        let get_field = |name: &str| -> Result<&Value> {
            config.get(name).ok_or_else(|| {
                LlamaError::config_error(name, format!("Missing required field: {}", name))
            })
        };

        let dim = get_field("hidden_size")?
            .as_u64()
            .ok_or_else(|| LlamaError::config_error("hidden_size", "Must be a number"))?
            as usize;

        let n_layers = get_field("num_hidden_layers")?
            .as_u64()
            .ok_or_else(|| LlamaError::config_error("num_hidden_layers", "Must be a number"))?
            as usize;

        let n_heads = get_field("num_attention_heads")?
            .as_u64()
            .ok_or_else(|| LlamaError::config_error("num_attention_heads", "Must be a number"))?
            as usize;

        let n_kv_heads = config
            .get("num_key_value_heads")
            .and_then(|v| v.as_u64())
            .map(|v| v as usize);

        let vocab_size = get_field("vocab_size")?
            .as_u64()
            .ok_or_else(|| LlamaError::config_error("vocab_size", "Must be a number"))?
            as usize;

        let intermediate_size = config
            .get("intermediate_size")
            .and_then(|v| v.as_u64())
            .unwrap_or(11008) as usize; // Default for most Llama models

        let rms_norm_eps = config
            .get("rms_norm_eps")
            .and_then(|v| v.as_f64())
            .unwrap_or(1e-5) as f32;

        let rope_theta = config
            .get("rope_theta")
            .and_then(|v| v.as_f64())
            .unwrap_or(10000.0) as f32;

        Ok(LlamaConfig {
            dim,
            n_layers,
            n_heads,
            n_kv_heads,
            vocab_size,
            ffn_dim_multiplier: None, // We calculate from intermediate_size
            intermediate_size,        // Use extracted value
            multiple_of: 256,         // Standard value
            norm_eps: rms_norm_eps,
            rope_theta,
            max_seq_len: 4096,      // Standard default
            use_scaled_rope: false, // Default value
        })
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_config_parsing() {
        let config_json = r#"{
            "architectures": ["LlamaForCausalLM"],
            "hidden_size": 4096,
            "intermediate_size": 14336,
            "num_attention_heads": 32,
            "num_hidden_layers": 32,
            "num_key_value_heads": 8,
            "vocab_size": 128256,
            "rms_norm_eps": 1e-05,
            "rope_theta": 500000.0
        }"#;

        let config_value: Value = serde_json::from_str(config_json).unwrap();
        let config = InfernoLlama::parse_config_from_json(config_value).unwrap();

        assert_eq!(config.dim, 4096);
        assert_eq!(config.n_layers, 32);
        assert_eq!(config.n_heads, 32);
        assert_eq!(config.n_kv_heads, Some(8));
        assert_eq!(config.vocab_size, 128256);
        assert_eq!(config.norm_eps, 1e-5);
        assert_eq!(config.rope_theta, 500000.0);
    }
}
