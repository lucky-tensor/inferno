[workspace]
members = [
    "crates/shared",
    "crates/proxy",
    "crates/backend",
    "crates/governator",
    "crates/cli",
    "crates/inference",
    "crates/inferno-llama",
    "benchmarking",
    "testsuite",
]
resolver = "2"
default-members = ["crates/cli"]

[workspace.package]
version = "0.1.0"
edition = "2021"
authors = ["Inferno Team"]
license = "Apache-2.0"
repository = "https://github.com/lucky-tensor/inferno"

[workspace.dependencies]
# Core Rust dependencies
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
serde_yaml = "0.9"
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
thiserror = "1.0"
async-trait = "0.1"
clap = { version = "4.0", features = ["derive", "env"] }
bytes = "1.0"
chrono = { version = "0.4", features = ["serde"] }

# Networking and HTTP dependencies
hyper = { version = "0.14", features = ["full"] }
hyper-v1 = { package = "hyper", version = "1.0", features = ["full"] }
http = "0.2"
reqwest = { version = "0.11", features = ["json", "blocking", "multipart", "stream"] }

# Configuration and environment
config = "0.14"
toml = "0.8"
validator = { version = "0.18", features = ["derive"] }

# System and utilities
num_cpus = "1.16"
parking_lot = "0.12"
once_cell = "1.19"
tempfile = "3.8"

# Pingora dependencies for proxy
pingora-core = "0.6"
pingora-proxy = "0.6"
pingora-http = "0.6"

# Development and testing dependencies
criterion = { version = "0.5", features = ["html_reports"] }
tokio-test = "0.4"
proptest = "1.4"
wiremock = "0.5"
serial_test = "3.0"
rstest = "0.18"

# Coverage dependencies
cargo-llvm-cov = "0.6"

# Database dependencies
sqlx = { version = "0.6", features = ["postgres", "chrono", "runtime-tokio-native-tls"] }

# Workspace internal dependencies
inferno-shared = { path = "crates/shared" }
inferno-proxy = { path = "crates/proxy" }
inferno-backend = { path = "crates/backend" }
inferno-governator = { path = "crates/governator" }
inferno-inference = { path = "crates/inference" }
inferno-llama = { path = "crates/inferno-llama" }

# ML Framework dependencies - Burn
burn = "0.18"
burn-core = "0.18"
burn-ndarray = "0.18"
burn-cuda = "0.18"
burn-tensor = "0.18"
burn-import = "0.18"
llama-burn = { git = "https://github.com/tracel-ai/models", package = "llama-burn", features = ["tiny", "pretrained"] }

# ML Framework dependencies - Candle
candle-core = { version = "0.9.1" }
candle-nn = { version = "0.9.1" }
candle-transformers = { version = "0.9.1" }

# ML and AI common dependencies
tokenizers = "0.15"
hf-hub = { version = "0.3", features = ["tokio"] }
safetensors = "0.4"
half = "2.3"
bytemuck = "1.0"

# CLI and download specific dependencies
anyhow = "1.0"
git2 = "0.19"
futures-util = "0.3"
sha2 = "0.10"
hex = "0.4"
indicatif = "0.17"
sysinfo = "0.30"
regex = "1.10"
walkdir = "2.4"
rustyline = "14.0"
rand = "0.8"

# Backend specific HTTP dependencies
hyper-util = { version = "0.1", features = ["full"] }
http-body-util = "0.1"


# Shared utilities and protocols dependencies (from inferno-shared)
futures = "0.3"
uuid = { version = "1.0", features = ["v4", "serde"] }
fastrand = "2.0"
bincode = "1.3"
zstd = "0.13"
dirs = "5.0"

# Dev dependencies for proxy testing
libc = "0.2"

[workspace.metadata.docs.rs]
all-features = true
rustdoc-args = ["--cfg", "docsrs"]


[profile.release]
codegen-units = 1
panic = "abort"
strip = true
debug = false
opt-level = 3
lto = "fat"

[profile.bench]
codegen-units = 1
strip = true
debug = false
opt-level = 3
lto = "fat"
